{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import main\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import shlex\n",
    "\n",
    "from evaluation import label_evaluation\n",
    "from utils.utils import mkdir\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from preprocessing import complete_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_label = np.load('dataset/SMD/SMD_test_label.npy')\n",
    "# test = np.load('dataset/SMD/SMD_test.npy')\n",
    "# train = np.load('dataset/SMD/SMD_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(preds, labels, kpi_id, missing, timestamp):\n",
    "\n",
    "    truth = pd.DataFrame({'missing': missing, 'label': labels, 'KPI ID': kpi_id, 'timestamp': timestamp})\n",
    "    truth[truth['missing'] == 0].to_hdf(truth_file, key='df')\n",
    "\n",
    "    prediction = pd.DataFrame({'missing': missing, 'predict': preds, 'KPI ID': kpi_id, 'timestamp': timestamp})\n",
    "    prediction[prediction['missing'] == 0].to_csv(result_file)\n",
    "\n",
    "    return json.loads(label_evaluation(truth_file, result_file, delay))['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_SIZE = 50\n",
    "\n",
    "ARG_STRING_TRAIN = '--anormly_ratio 2.1 --num_epochs 1 --batch_size 32 --mode train --dataset ALL-50 --data_path dataset/ALL-50 --input_c 1'\n",
    "ARG_STRING_TEST = '--anormly_ratio 2.1 --num_epochs 1 --batch_size 32 --mode test --dataset ALL-50 --data_path dataset/ALL-50 --input_c 1 --pretrained_model 20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=1e-4)\n",
    "parser.add_argument('--num_epochs', type=int, default=10)\n",
    "parser.add_argument('--k', type=int, default=3)\n",
    "parser.add_argument('--win_size', type=int, default=WIN_SIZE)\n",
    "parser.add_argument('--input_c', type=int, default=1)\n",
    "parser.add_argument('--output_c', type=int, default=1)\n",
    "parser.add_argument('--batch_size', type=int, default=1024)\n",
    "parser.add_argument('--pretrained_model', type=str, default=None)\n",
    "parser.add_argument('--dataset', type=str, default='credit')\n",
    "parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])\n",
    "parser.add_argument('--data_path', type=str, default='./dataset/creditcard_ts.csv')\n",
    "parser.add_argument('--model_save_path', type=str, default='checkpoints')\n",
    "parser.add_argument('--anormly_ratio', type=float, default=4.00)\n",
    "\n",
    "config_train = parser.parse_args(shlex.split(ARG_STRING_TRAIN))\n",
    "config_test = parser.parse_args(shlex.split(ARG_STRING_TEST))\n",
    "\n",
    "solver_train = main(config_train)\n",
    "solver_test = main(config_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../train.csv'\n",
    "test_file = '../test.csv'\n",
    "\n",
    "result_file = 'predict.csv'\n",
    "truth_file = 'ground_truth.hdf'\n",
    "delay = 7\n",
    "\n",
    "train_df = pd.read_csv(train_file).rename(columns={'KPI ID': 'kpi_id'})\n",
    "test_df = pd.read_csv(test_file).rename(columns={'KPI ID': 'kpi_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "for kpi_id in train_df.kpi_id.unique():\n",
    "    kpi = train_df[train_df['kpi_id'] == kpi_id]\n",
    "    t_kpi = test_df[test_df['kpi_id'] == kpi_id]\n",
    "    t_kpi['label'] = 0\n",
    "\n",
    "    train_timestamp, train_missing, (train_values, train_labels) = complete_timestamp(kpi['timestamp'], (kpi['value'], kpi['label']))\n",
    "    test_timestamp, test_missing, (test_values, test_labels) = complete_timestamp(t_kpi['timestamp'], (t_kpi['value'], t_kpi['label']))\n",
    "\n",
    "    window_miss_train = ((len(train_timestamp) // WIN_SIZE + 1) * WIN_SIZE) - len(train_timestamp)\n",
    "\n",
    "    last_timestamp = train_timestamp[-1]\n",
    "    final_timestamp = last_timestamp + (60*window_miss_train)\n",
    "\n",
    "    train_timestamp = np.append(train_timestamp, np.arange(last_timestamp+60, final_timestamp+1, 60))\n",
    "    train_missing = np.append(train_missing, [1]*window_miss_train)\n",
    "    train_values = np.append(train_values, [0.]*window_miss_train)\n",
    "    train_labels = np.append(train_labels, [0]*window_miss_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_values.reshape(-1, 1))\n",
    "\n",
    "    window_miss_test = ((len(test_timestamp) // WIN_SIZE + 1) * WIN_SIZE) - len(test_timestamp)\n",
    "\n",
    "    last_timestamp = test_timestamp[-1]\n",
    "    final_timestamp = last_timestamp + (60*window_miss_test)\n",
    "\n",
    "    test_timestamp = np.append(test_timestamp, np.arange(last_timestamp+60, final_timestamp+1, 60))\n",
    "    test_missing = np.append(test_missing, [1]*window_miss_test)\n",
    "    test_values = np.append(test_values, [0.]*window_miss_test)\n",
    "    test_labels = np.append(test_labels, [0]*window_miss_test)\n",
    "\n",
    "    df_test = pd.DataFrame({'timestamp': test_timestamp, 'missing': test_missing, 'value': scaler.transform(test_values.reshape(-1, 1))[:, 0], 'label': test_labels})\n",
    "\n",
    "    np.save(f'dataset/ALL-50/ALL-50_test.npy', df_test['value'].to_numpy().reshape(-1,1))\n",
    "    np.save(f'dataset/ALL-50/ALL-50_test_label.npy', df_test['label'].to_numpy())\n",
    "\n",
    "    solver_test = main(config_test)\n",
    "\n",
    "    kpi_pred = pd.DataFrame({'missing': test_missing, 'predict': solver_test.pred, 'KPI ID': kpi_id, 'timestamp': test_timestamp})\n",
    "    predictions = predictions.append(kpi_pred[kpi_pred['missing'] == 0].drop(['missing'], axis=1)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_file).rename(columns={'KPI ID': 'kpi_id'})\n",
    "test_df = pd.read_csv(test_file).rename(columns={'KPI ID': 'kpi_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame()\n",
    "for kpi_id in train_df.kpi_id.unique():\n",
    "    kpi = train_df[train_df['kpi_id'] == kpi_id]\n",
    "    t_kpi = test_df[test_df['kpi_id'] == kpi_id]\n",
    "    t_kpi['label'] = 0\n",
    "\n",
    "    train_timestamp, train_missing, (train_values, train_labels) = complete_timestamp(kpi['timestamp'], (kpi['value'], kpi['label']))\n",
    "    test_timestamp, test_missing, (test_values, test_labels) = complete_timestamp(t_kpi['timestamp'], (t_kpi['value'], t_kpi['label']))\n",
    "\n",
    "    window_miss_train = ((len(train_timestamp) // WIN_SIZE + 1) * WIN_SIZE) - len(train_timestamp)\n",
    "\n",
    "    last_timestamp = train_timestamp[-1]\n",
    "    final_timestamp = last_timestamp + (60*window_miss_train)\n",
    "\n",
    "    train_timestamp = np.append(train_timestamp, np.arange(last_timestamp+60, final_timestamp+1, 60))\n",
    "    train_missing = np.append(train_missing, [1]*window_miss_train)\n",
    "    train_values = np.append(train_values, [0.]*window_miss_train)\n",
    "    train_labels = np.append(train_labels, [0]*window_miss_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_values.reshape(-1, 1))\n",
    "\n",
    "    window_miss_test = ((len(test_timestamp) // WIN_SIZE + 1) * WIN_SIZE) - len(test_timestamp)\n",
    "\n",
    "    last_timestamp = test_timestamp[-1]\n",
    "    final_timestamp = last_timestamp + (60*window_miss_test)\n",
    "\n",
    "    test_timestamp = np.append(test_timestamp, np.arange(last_timestamp+60, final_timestamp+1, 60))\n",
    "    test_missing = np.append(test_missing, [1]*window_miss_test)\n",
    "    test_values = np.append(test_values, [0.]*window_miss_test)\n",
    "    test_labels = np.append(test_labels, [0]*window_miss_test)\n",
    "\n",
    "    df_train = pd.DataFrame({'timestamp': train_timestamp, 'missing': train_missing, 'value': scaler.transform(train_values.reshape(-1, 1))[:, 0], 'label': train_labels})\n",
    "    df_test = pd.DataFrame({'timestamp': test_timestamp, 'missing': test_missing, 'value': scaler.transform(test_values.reshape(-1, 1))[:, 0], 'label': test_labels})\n",
    "\n",
    "    all_df = all_df.append(df_train).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(f'dataset/ALL-50')\n",
    "np.save(f'dataset/ALL-50/ALL-50_train.npy', all_df['value'].to_numpy().reshape(-1,1))\n",
    "np.save(f'dataset/ALL-50/ALL-50_test.npy', all_df['value'].to_numpy().reshape(-1,1))\n",
    "np.save(f'dataset/ALL-50/ALL-50_test_label.npy', all_df['label'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "699ce8c1c1ef11a8038e3eb42d83098f539028eddb6d9ae6528bef3cc6016601"
  },
  "kernelspec": {
   "display_name": "Python 3.6.6 ('AT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
