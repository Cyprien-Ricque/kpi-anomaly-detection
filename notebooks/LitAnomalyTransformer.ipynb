{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from data_factory.DataLoader import DataLoader\n",
    "from utils.config import load_config\n",
    "# evaluation file\n",
    "from utils.evaluation import label_evaluation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('/media/cyprien/Data/Documents/Github/pytorch-forecasting')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "config_file = \"../config/config.yml\"\n",
    "config = load_config(config_file)\n",
    "\n",
    "result_file = '../predict.csv'\n",
    "truth_file = '../ground_truth.hdf'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_factory.DataLoader:Use previously generated file ../data//data_export_train.csv_test.csv_fmd-True_False_True_True_0.95.p. Can not redo preprocessing by loading from generated file.\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(use_previous_files=True, config_file=config_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "max_prediction_length = 1\n",
    "max_encoder_length = config['AnomalyTransformer']['max_seq_len']\n",
    "min_encoder_length = config['AnomalyTransformer']['min_seq_len']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "X_cols = ['value_scaled', 'kpi_id', 'timestamp_1', 'authentic']\n",
    "normal_train = dl.train.reset_index(drop=True)\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    normal_train.loc[:, X_cols],\n",
    "    time_idx='timestamp_1', target='value_scaled',\n",
    "    group_ids=['kpi_id'],\n",
    "    allow_missing_timesteps=False,\n",
    "    static_categoricals=['kpi_id', 'authentic'],\n",
    "    time_varying_unknown_reals=['value_scaled'],\n",
    "    # time_varying_known_reals=['timestamp_1'],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_encoder_length=min_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    scalers={col: None for col in ['timestamp_1', 'kpi_id']},\n",
    "    target_normalizer=None,\n",
    "    add_relative_time_idx=False,\n",
    "    add_target_scales=False,\n",
    "    add_encoder_length=False,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, dl.val.loc[:, X_cols], stop_randomization=True, predict=False\n",
    ")\n",
    "testing = TimeSeriesDataSet.from_dataset(\n",
    "    training, dl.test.loc[:, X_cols], stop_randomization=True, predict=False, min_encoder_length=max_encoder_length\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "training_dl = training.to_dataloader(train=True, batch_size=batch_size, num_workers=12)\n",
    "\n",
    "validation_dl = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=12)\n",
    "testing_dl = testing.to_dataloader(train=False, batch_size=batch_size * 3, num_workers=12)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from models.AnomalyTransformer.AnomalyTransformer import AnomalyTransformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)\n",
    "n_features = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del trainer\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpiqec14bp\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpiqec14bp/_remote_module_non_sriptable.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = AnomalyTransformer(\n",
    "    win_size=max_encoder_length, enc_in=n_features, c_out=1,\n",
    "    d_model=256, n_heads=4, e_layers=2, d_ff=256,\n",
    "    dropout=0.0, activation='gelu', output_attention=True\n",
    ")\n",
    "\n",
    "# model = LitAE.load_from_checkpoint(\"./lightning_logs/version_1/checkpoints/epoch=4-step=171145.ckpt\",\n",
    "#                                    input_shape=max_encoder_length, n_dim=n_dim)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True, used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a7e691c04bd4b31ace973def0739ff1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     val_loss_epoch          3.32279896736145\n",
      "   val_max_loss_epoch        25.94574737548828\n",
      "   val_min_loss_epoch       -22.622953414916992\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'val_loss_epoch': 3.32279896736145,\n  'val_min_loss_epoch': -22.622953414916992,\n  'val_max_loss_epoch': 25.94574737548828}]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(logger=True, enable_checkpointing=True, checkpoint_callback=None, gpus=1, auto_lr_find=True, max_epochs=-1)\n",
    "\n",
    "trainer.validate(model=model, dataloaders=validation_dl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name       | Type          | Params\n",
      "---------------------------------------------\n",
      "0 | embedding  | DataEmbedding | 768   \n",
      "1 | encoder    | Encoder       | 795 K \n",
      "2 | projection | Linear        | 257   \n",
      "---------------------------------------------\n",
      "796 K     Trainable params\n",
      "0         Non-trainable params\n",
      "796 K     Total params\n",
      "3.185     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "457187ce87404125b5199b4f4f62dbdd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1fb70e389fb4e1b902142a5181aa3af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model=model, train_dataloaders=training_dl, val_dataloaders=validation_dl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.validate(model=model, dataloaders=validation_dl)\n",
    "\n",
    "# trainer.test(model)\n",
    "# trainer.predict(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict abnormal value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def predict_value(df: pd.DataFrame):\n",
    "    df['timestamp_1_floor'] = df.groupby('kpi_id').timestamp_1.transform(lambda x: x - x.min())\n",
    "    pv = pd.pivot_table(df, values='value_scaled', index='timestamp_1_floor', columns='kpi_id', fill_value=np.nan)\n",
    "\n",
    "    ept = np.empty(max_encoder_length)\n",
    "    ept[:] = np.nan\n",
    "\n",
    "    pv = pd.concat([pv, pd.DataFrame({col: ept.copy() for col in pv.columns})])\n",
    "\n",
    "    pv_forward = np.moveaxis(pv.copy().to_numpy(dtype=np.float32), 1, 0)\n",
    "    pv_filter = np.moveaxis(pv.copy().to_numpy(), 1, 0)\n",
    "    pv_forward[np.isnan(pv_forward)] = 0\n",
    "    pv_filter = np.where(np.isnan(pv_filter), 0, 1)\n",
    "\n",
    "    df['value_pred'] = np.nan\n",
    "\n",
    "    for i in tqdm(np.arange(0, pv.index.max() + 1, max_encoder_length)):\n",
    "        pv_forward_i = pv_forward[:, i:i + max_encoder_length]\n",
    "        pv_filter_i = pv_filter[:, i:i + max_encoder_length]\n",
    "\n",
    "        X = torch.from_numpy(pv_forward_i.copy())\n",
    "        X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "        y_pred = model(X)\n",
    "\n",
    "        validation_filter = df.groupby('kpi_id').apply(\n",
    "            lambda x: (x.timestamp_1_floor >= i) & (x.timestamp_1_floor < i + max_encoder_length)).reset_index(\n",
    "            drop=True)\n",
    "\n",
    "        df.loc[validation_filter, 'value_pred'] = y_pred.cpu().detach().numpy()[pv_filter_i.astype(bool)].flatten()\n",
    "\n",
    "\n",
    "predict_value(dl.train)\n",
    "predict_value(dl.test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_label(df: pd.DataFrame):\n",
    "\n",
    "    df['label_pred'] = np.nan\n",
    "\n",
    "    for kpi_id in tqdm(df.kpi_id.unique()):\n",
    "        THRESHOLD = np.abs(df[(df.kpi_id == kpi_id)].value_pred - df[(df.kpi_id == kpi_id)].value_scaled).quantile(.991)\n",
    "        df.loc[(df.kpi_id == kpi_id), 'label_pred'] = np.abs(\n",
    "            df[(df.kpi_id == kpi_id)].value_pred -\n",
    "            df[(df.kpi_id == kpi_id)].value_scaled) > THRESHOLD\n",
    "\n",
    "\n",
    "predict_label(dl.train)\n",
    "predict_label(dl.test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import WARNING\n",
    "\n",
    "logging.basicConfig(level=WARNING)\n",
    "\n",
    "for kpi_id in dl.train.kpi_id.unique()[:3]:\n",
    "    df = dl.train[(dl.train.kpi_id == kpi_id) & (~validation_filter)]\n",
    "\n",
    "    figure = plt.figure(figsize=(20, 4))\n",
    "    plt.hist(np.abs(df.value_pred - df.value_scaled), bins=200, figure=figure)\n",
    "\n",
    "    true_positive = ((df.label == 1) & (df.label_pred == 1))\n",
    "    false_positive = ((df.label == 0) & (df.label_pred == 1))\n",
    "    false_negative = ((df.label == 1) & (df.label_pred == 0))\n",
    "\n",
    "    figure = plt.figure(figsize=(20, 8))\n",
    "    for filter, color in zip([true_positive, false_positive, false_negative], ['g', 'orange', 'r']):\n",
    "        plt.scatter(df[filter].timestamp_1, df[filter].value_scaled, c=color, alpha=.4)\n",
    "\n",
    "    plt.plot(df.timestamp_1, df.value_scaled, figure=figure, linewidth=.5)\n",
    "    plt.plot(df.timestamp_1, df.value_pred, figure=figure, linewidth=.5)\n",
    "    plt.title(f'kpi id: {kpi_id}')\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import WARNING\n",
    "\n",
    "logging.basicConfig(level=WARNING)\n",
    "\n",
    "for kpi_id in dl.train.kpi_id.unique()[:3]:\n",
    "    df = dl.train[(dl.train.kpi_id == kpi_id) & (validation_filter)]\n",
    "\n",
    "    figure = plt.figure(figsize=(20, 4))\n",
    "    plt.hist(np.abs(df.value_pred - df.value_scaled), bins=200, figure=figure)\n",
    "\n",
    "    true_positive = ((df.label == 1) & (df.label_pred == 1))\n",
    "    false_positive = ((df.label == 0) & (df.label_pred == 1))\n",
    "    false_negative = ((df.label == 1) & (df.label_pred == 0))\n",
    "\n",
    "    figure = plt.figure(figsize=(20, 8))\n",
    "    for filter, color in zip([true_positive, false_positive, false_negative], ['g', 'orange', 'r']):\n",
    "        plt.scatter(df[filter].timestamp_1, df[filter].value_scaled, c=color, alpha=.4)\n",
    "\n",
    "    plt.plot(df.timestamp_1, df.value_scaled, figure=figure, linewidth=.5)\n",
    "    plt.plot(df.timestamp_1, df.value_pred, figure=figure, linewidth=.5)\n",
    "    plt.title(f'kpi id: {kpi_id}')\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validation_start_points = normal_train.groupby('kpi_id').apply(\n",
    "    lambda x: x.timestamp_1.max() - (max_encoder_length + max_prediction_length)).to_frame('limit').to_dict()['limit']\n",
    "validation_start_points"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validation_filter = dl.train.groupby('kpi_id').apply(\n",
    "    lambda x: x.timestamp_1 >= validation_start_points[x.kpi_id.iloc[0]]).reset_index(\n",
    "    drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = dl.train.loc[validation_filter  & (dl.train.authentic == 'True'), ['timestamp', 'kpi_id', 'label_pred']].rename(columns={'kpi_id': 'KPI ID'})\n",
    "prediction['predict'] = prediction['label_pred']\n",
    "prediction.to_csv(result_file)\n",
    "\n",
    "ground_truth = dl.train.loc[validation_filter & (dl.train.authentic == 'True'), ['timestamp', 'kpi_id', 'label']].rename(columns={'kpi_id': 'KPI ID'})\n",
    "ground_truth.to_hdf(truth_file, key='df')\n",
    "\n",
    "print(label_evaluation(truth_file, result_file))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = dl.train.loc[~validation_filter & (dl.train.authentic == 'True'), ['timestamp', 'kpi_id', 'label_pred']].rename(columns={'kpi_id': 'KPI ID'})\n",
    "prediction['predict'] = prediction['label_pred']\n",
    "prediction.to_csv(result_file)\n",
    "\n",
    "ground_truth = dl.train.loc[~validation_filter & (dl.train.authentic == 'True'), ['timestamp', 'kpi_id', 'label']].rename(columns={'kpi_id': 'KPI ID'})\n",
    "ground_truth.to_hdf(truth_file, key='df')\n",
    "\n",
    "print(label_evaluation(truth_file, result_file))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    ".997: .2791\n",
    ".995: .4579\n",
    ".993: .4628\n",
    ".992: .5719\n",
    ".991: .5609\n",
    ".990: .5525\n",
    ".989: .5521\n",
    ".985: .5469"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = dl.test.loc[dl.test.authentic == 'True', ['timestamp', 'kpi_id', 'label_pred']].rename(columns={'kpi_id': 'KPI ID'})\n",
    "prediction['predict'] = prediction['label_pred'].astype(int)\n",
    "prediction.drop(columns=['label_pred'], inplace=True)\n",
    "prediction.to_csv(result_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import WARNING\n",
    "\n",
    "logging.basicConfig(level=WARNING)\n",
    "\n",
    "for kpi_id in dl.test.kpi_id.unique()[:10]:\n",
    "    df = dl.test[(dl.test.kpi_id == kpi_id) & (dl.test.authentic == 'True')]\n",
    "\n",
    "    figure = plt.figure(figsize=(20, 4))\n",
    "    plt.hist(np.abs(df.value_pred - df.value_scaled), bins=200, figure=figure)\n",
    "    plt.show()\n",
    "\n",
    "    figure = plt.figure(figsize=(20, 4))\n",
    "\n",
    "    plt.scatter(df[df.label_pred == 1].timestamp_1, df[df.label_pred == 1].value_scaled, c='r', alpha=.4)\n",
    "\n",
    "    plt.plot(df.timestamp_1, df.value_scaled, figure=figure, linewidth=.5)\n",
    "    plt.plot(df.timestamp_1, df.value_pred, figure=figure, linewidth=.5)\n",
    "    plt.title(f'kpi id: {kpi_id}')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d4133a6686e7b6e1653cde6574acd837d64e50b91f09c2e2fee9163e88bab39"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('THU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}