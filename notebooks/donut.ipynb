{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from donut import complete_timestamp, standardize_kpi\n",
    "from donut import DonutTrainer, DonutPredictor\n",
    "from donut import Donut\n",
    "\n",
    "from tfsnippet.utils import get_variables_as_dict, VariableSaver\n",
    "from tfsnippet.modules import Sequential\n",
    "from evaluation import label_evaluation\n",
    "from tensorflow import keras as K\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../train.csv'\n",
    "test_file = '../test.csv'\n",
    "\n",
    "result_file = 'predict.csv'\n",
    "truth_file = 'ground_truth.hdf'\n",
    "delay = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_file).rename(columns={'KPI ID': 'kpi_id'})\n",
    "test_df = pd.read_csv(test_file).rename(columns={'KPI ID': 'kpi_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.kpi_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_id = '02e99bd4f6cfb33f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi = train_df[train_df['kpi_id'] == kpi_id]\n",
    "t_kpi = test_df[test_df['kpi_id'] == kpi_id]\n",
    "t_kpi['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timestamp, train_missing, (train_values, train_labels) = complete_timestamp(kpi['timestamp'], (kpi['value'], kpi['label']))\n",
    "test_timestamp, test_missing, (test_values, test_labels) = complete_timestamp(t_kpi['timestamp'], (t_kpi['value'], t_kpi['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values, mean, std = standardize_kpi(\n",
    "    train_values, excludes=np.logical_or(train_labels, train_missing))\n",
    "test_values, _, _ = standardize_kpi(test_values, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLIDING_WINDOW = 120\n",
    "\n",
    "with tf.variable_scope(kpi_id) as model_vs:\n",
    "    model = Donut(\n",
    "        h_for_p_x=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        h_for_q_z=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        x_dims=SLIDING_WINDOW,\n",
    "        z_dims=5,\n",
    "    )\n",
    "\n",
    "trainer = DonutTrainer(model=model, model_vs=model_vs)\n",
    "predictor = DonutPredictor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/{}/\".format(kpi_id)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "saved = True\n",
    "if len(os.listdir(save_dir)) == 0:\n",
    "    saved = False\n",
    " \n",
    "if saved is False:\n",
    "    with tf.Session().as_default():\n",
    "        # train the model\n",
    "        trainer.fit(train_values, train_labels, train_missing, mean, std)\n",
    "        # save variables to 'save_dir' directory\n",
    "        var_dict = get_variables_as_dict(model_vs)\n",
    "        saver = VariableSaver(var_dict, save_dir)\n",
    "        saver.save()\n",
    "        saved = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(preds, labels, kpi_id, missing, timestamp):\n",
    "\n",
    "    truth = pd.DataFrame({'missing': missing, 'label': labels, 'KPI ID': kpi_id, 'timestamp': timestamp})\n",
    "    truth[truth['missing'] == 0].to_hdf(truth_file, key='df')\n",
    "\n",
    "    prediction = pd.DataFrame({'missing': missing, 'predict': preds, 'KPI ID': kpi_id, 'timestamp': timestamp})\n",
    "    prediction[prediction['missing'] == 0].to_csv(result_file)\n",
    "\n",
    "    return json.loads(label_evaluation(truth_file, result_file, delay))['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donut train and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session().as_default():\n",
    "    # restore variables from 'save_dir'\n",
    "    saver = VariableSaver(get_variables_as_dict(kpi_id), save_dir)\n",
    "    saver.restore()\n",
    "    # make predictions\n",
    "    train_score = predictor.get_score(train_values, train_missing)\n",
    "    test_score = predictor.get_score(test_values, test_missing)\n",
    "    # try different thresholds\n",
    "    best_threshold, best_f1, best_predictions = 0, 0, []\n",
    "    thresholds = np.arange(0, 50, 0.2)\n",
    "\n",
    "    for t in thresholds:\n",
    "        anomaly_predictions = np.where(abs(train_score) > t, 1, 0)\n",
    "        f1 = evaluate(np.concatenate([[0]*(SLIDING_WINDOW-1), anomaly_predictions]), train_labels, kpi_id, train_missing, train_timestamp)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = t\n",
    "            best_predictions = anomaly_predictions\n",
    "\n",
    "    anomaly_predictions = best_predictions\n",
    "    print(\"Best f1 score: {}\".format(best_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.where(abs(test_score) > best_threshold, 1, 0)\n",
    "predictions = pd.DataFrame({'missing': test_missing, 'predict': np.concatenate([[0]*(SLIDING_WINDOW-1), predictions]), 'KPI ID': kpi_id, 'timestamp': test_timestamp})\n",
    "predictions[predictions['missing'] == 0].to_csv('{}.csv'.format(kpi_id))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "340401db7bd54dda0d508c5fe27bf037e241382128cae0e0efc7461c761a9997"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('interfusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
